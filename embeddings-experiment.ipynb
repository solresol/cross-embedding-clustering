{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3369cdd",
   "metadata": {},
   "source": [
    "# Embeddings Comparison\n",
    "\n",
    "The idea is that we want to see if BERT produces ultrametric-like embeddings. Probabilistically, it should.\n",
    "\n",
    "We create NUMBER_OF_TRIALS trials. Each trial has between FEWEST_EXEMPLARS and MOST_EXAMPLARS exemplars randomly\n",
    "selected from monosemous (single-meaning) words.\n",
    "\n",
    "For each trial we look at the whole vocabulary, and assign each word in the vocabulary to the nearest exemplar.\n",
    "We do this using wordnet (path similarity) and BERT (using Euclidean distance). Hopefully one day I'll turn this\n",
    "into something p-adic too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f60c98",
   "metadata": {},
   "source": [
    "Should I use Resnik similarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "641ba781",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_TRIALS = 100\n",
    "FEWEST_EXEMPLARS = 5\n",
    "MOST_EXEMPLARS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b73f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import random\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import tqdm\n",
    "import collections\n",
    "from scipy.spatial.distance import cosine\n",
    "import pandas\n",
    "import matplotlib.pyplot\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e4d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
    "    \"\"\"Copied from https://towardsdatascience.com/3-types-of-contextualized-word-embeddings-from-bert-using-transfer-learning-81fcefe3fe6d\n",
    "    \n",
    "    Get embeddings from an embedding model\n",
    "    \n",
    "    Args:\n",
    "        tokens_tensor (obj): Torch tensor size [n_tokens]\n",
    "            with token ids for each token in text\n",
    "        segments_tensors (obj): Torch tensor size [n_tokens]\n",
    "            with segment ids for each token in text\n",
    "        model (obj): Embedding model to generate embeddings\n",
    "            from token and segment ids\n",
    "    \n",
    "    Returns:\n",
    "        list: List of list of floats of size\n",
    "            [n_tokens, n_embedding_dimensions]\n",
    "            containing embeddings for each token\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Gradient calculation id disabled\n",
    "    # Model is in inference mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Removing the first hidden state\n",
    "        # The first state is the input state\n",
    "        hidden_states = outputs[2][1:]\n",
    "\n",
    "    # Getting embeddings from the final BERT layer\n",
    "    token_embeddings = hidden_states[-1]\n",
    "    # Collapsing the tensor into 1-dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
    "    # Converting torchtensors to lists\n",
    "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
    "\n",
    "    return list_token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23259412",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af3bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753dda04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "monosemous_nouns = {}\n",
    "for word in wn.all_lemma_names():\n",
    "    synsets = wn.synsets(word)\n",
    "    if len(synsets) == 1 and synsets[0].pos() == 'n':\n",
    "        word_fragments = word.split('_')\n",
    "        all_fragments_in_bert = True\n",
    "        for fragment in word_fragments:\n",
    "            if word not in tokenizer.vocab:\n",
    "                all_fragments_in_bert = False\n",
    "                break\n",
    "        if not all_fragments_in_bert:\n",
    "            continue\n",
    "        monosemous_nouns[word] = synsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b64f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "monosemous_nouns_list = list(monosemous_nouns)\n",
    "len(monosemous_nouns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db61a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#working_subset_of_nouns = random.sample(monosemous_nouns_list, MOST_EXEMPLARS * 5)\n",
    "working_subset_of_nouns = monosemous_nouns_list[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df34922",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings = {}\n",
    "for word in working_subset_of_nouns:\n",
    "    marked_text = \"[CLS] \"\n",
    "    marked_text += word.replace('_', ' ')\n",
    "    marked_text += \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "    bert_embeddings[word] = list_token_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab83759",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = []\n",
    "for trial_number in tqdm.tqdm(range(NUMBER_OF_TRIALS)):\n",
    "    exemplar_count = random.randrange(FEWEST_EXEMPLARS, MOST_EXEMPLARS)\n",
    "    exemplar_words = random.sample(working_subset_of_nouns, k=exemplar_count)\n",
    "    exemplar_synsets = [monosemous_nouns[w] for w in exemplar_words]\n",
    "    exemplars = {w:s for (w,s) in zip(exemplar_words, exemplar_synsets)}\n",
    "    exemplar_embeddings = {w:bert_embeddings[w] for w in exemplar_words}\n",
    "    wordnet_neighbours = {}\n",
    "    bert_cosine_neighbours = {}\n",
    "    for word, synset in monosemous_nouns.items():\n",
    "        if word not in working_subset_of_nouns:\n",
    "            continue\n",
    "        best_exemplar = None\n",
    "        best_score = 0.0\n",
    "        for e_word, e_synset in exemplars.items():\n",
    "            similarity = max(wn.path_similarity(synset, e_synset), wn.path_similarity(e_synset, synset))\n",
    "            if similarity > best_score:\n",
    "                best_exemplar = e_word\n",
    "                best_score = similarity\n",
    "        wordnet_neighbours[word] = best_exemplar\n",
    "        best_exemplar = None\n",
    "        best_score = 0.0\n",
    "        word_embedding = bert_embeddings[word]\n",
    "        for e_word, e_embedding in exemplar_embeddings.items():\n",
    "            distance = 1 - cosine(word_embedding, e_embedding)\n",
    "            if distance > best_score:\n",
    "                best_exemplar = e_word\n",
    "                best_score = distance\n",
    "        bert_cosine_neighbours[word] = best_exemplar\n",
    "    trial = {'exemplar_count': exemplar_count,\n",
    "            'exemplar_words': exemplar_words,\n",
    "            'wordnet_neighbours': wordnet_neighbours,\n",
    "            'bert_cosine_neighbours': bert_cosine_neighbours}\n",
    "    trials.append(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd28b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.Series([x['exemplar_count'] for x in trials]).plot.hist(title=\"Number of exemplars in series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037f2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbour_analysis(trials, neighbour_key):\n",
    "    neighbour_dicts = [x[neighbour_key] for x in trials]\n",
    "    word_similarities = []\n",
    "    # I could perhaps be more efficient here. But quadratic time for <100,000 entries is not super-terrible.\n",
    "    for w1 in working_subset_of_nouns:\n",
    "        for w2 in working_subset_of_nouns:\n",
    "            if w2 <= w1:\n",
    "                continue\n",
    "            coexemplar_count = 0\n",
    "            for d in neighbour_dicts:\n",
    "                if d[w1] == d[w2]:\n",
    "                    coexemplar_count += 1\n",
    "            if coexemplar_count > 0:\n",
    "                word_similarities.append({'word1': w1, 'word2': w2, 'coexemplar_count': coexemplar_count})\n",
    "    return pandas.DataFrame.from_records(word_similarities)\n",
    "    #inverted_dicts = []\n",
    "    #for d in neighbour_dicts:\n",
    "    #    this_inverted_dict = collections.defaultdict(set)\n",
    "    #    for k,v in d.items():\n",
    "    #        this_inverted_dict[v].update([k])\n",
    "    #    inverted_dicts.append(this_inverted_dict)\n",
    "    #return inverted_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6eaaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_df = neighbour_analysis(trials, 'wordnet_neighbours').rename(columns={'coexemplar_count': 'wordnet_exemplar_count'})\n",
    "bert_cosine_df = neighbour_analysis(trials, 'bert_cosine_neighbours').rename(\n",
    "    columns={'coexemplar_count': 'bert_cosine_exemplar_count'})\n",
    "df = wn_df.merge(bert_cosine_df, how=\"outer\", on=['word1', 'word2']).fillna(0)\n",
    "df.sample(10, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b5372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = matplotlib.pyplot.subplots(ncols=2, figsize=(16,6))\n",
    "df.bert_cosine_exemplar_count.plot.hist(logy=True, ax=axes[0])\n",
    "axes[0].set_title(\"BERT distribution of neighbour affinity counts\")\n",
    "df.wordnet_exemplar_count.plot.hist(logy=True, ax=axes[1])\n",
    "axes[1].set_title(\"Wordnet distribution of neighbour affinity counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e1fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='bert_cosine_exemplar_count', y='wordnet_exemplar_count', alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7694f0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.kdeplot(df.bert_cosine_exemplar_count, df.wordnet_exemplar_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80db104",
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.lmplot(data=df,  x='bert_cosine_exemplar_count', y='wordnet_exemplar_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50023f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.bert_cosine_exemplar_count == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce16a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.wordnet_exemplar_count == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e486af4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
